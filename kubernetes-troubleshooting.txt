#--- TROUBLESHOOTING NOTES
#LINKS:
https://learnk8s.io/troubleshooting-deployments #important
https://laurinevala.medium.com/visualizing-kubernetes-resources-ee9d8c16d264  #proyect that create visual maps of Pods
https://grapeup.com/blog/common-kubernetes-failures-at-scale/
https://www.itprotoday.com/hybrid-cloud/8-problems-kubernetes-architecture

https://blog.pipetail.io/posts/2020-05-04-most-common-mistakes-k8s/
- resources - requests and limits -> OOMkill
- liveness and readiness probes
- LoadBalancer for every http service
- non-kubernetes-aware cluster autoscaling
- Not using the power of IAM/RBAC
- self anti-affinities for pods
- no poddisruptionbudget -> pod upgrade
- more tenants or envs in shared cluster
- externalTrafficPolicy: Cluster
- pet clusters + stressing the control plane too much

# With Pods, start for drawing the schema if you know it: DB-pod -> DB-service -> WEB-pod -> WEB-service
#Check the logs of the pod with "kubectl logs POD" then if it is "restarting" check the live log with the option "-f"
#You can even check the logs of the previous pod with option "--previous"

#If the Control Plane components are deployed as pods (kube-apiserver, control-manager, kube-schduler) check the pods
#with "kubectl get pods -n kube-systemctl". In case the controlplane are deployed as services check the service status
#of them in the master, example: 
service kube-apiserver status
        control-manager status
        kube-scheduler status
#and the kubelet and kube-proxy in the worker nodes:
service kubelet status
        kube-proxy status
#then check the logs:
kubectl logs POD-master -n kube-system
sudo journalctl -u kube-apiserver #in case it is a service host logging solution

#For node just use the describe option, to keep in mind the status:
#FALSE
#TRUE
#UNKNOWN > possible lost connection between worker and master
#and dont forget to check the certificates:
openssl x509 -in /path/to/certificate.crt -text #to check the expiration time

#TSHOOT NODES - STATICPOD ISSUE
#find the configuration file, first ls inside the folder then find the kubeadm config file
ls /etc/systemd/system/kubelet.service.d/
cat /etc/systemd/system/kubelet.service.d/FILE
#you will find the path in the --config=/path/to/configfile. now grep the search
grep -i staticPodPath /pat/to/configfile.yaml
#move to the folder and fix the error, the pods will restart after a configuration change.

#TROUBLESHOOTING - LOOKING AT LOGS
Master 
/var/log/kube-apiserver.log - API Server, responsible for serving the API
/var/log/kube-scheduler.log - Scheduler, responsible for making scheduling decisions
/var/log/kube-controller-manager.log - Controller that manages replication controllers

Worker Nodes
/var/log/kubelet.log - Kubelet, responsible for running containers on the node
/var/log/kube-proxy.log - Kube Proxy, responsible for service load balancing
On workers, SSH into them and check the systemctl:
systemctl status kubelet/kube-proxy

take this path in consideration: "/etc/systemd/system/kubelet.service.d/" you should find there the kubeadm config.
take a look on the "--config=PATH" and check in that yaml config file the configuration (example, wrong CA.crt path)
#TROUBLESHOOT NOTES APART - EASY AND CONCISE
Connecting deployment and service:
The Service spec.selector should match at least one Pod template.metadata.labels.name
The Service spec.ports.port.targetPort should match the spec.containers.ports.containerPort of the Pod.
The Service port can be any number. Multiple Services can use the same port because they have different IP addresses assigned.

Connecting Service and Ingress:
Two things should match in the Ingress and Service:
The spec././.service.port of the Ingress should match the spec.ports.port of the Service
The spec././.service.name of the Ingress should match the metadata.name of the Service

#TROUBLESHOOT NOTES NETWORKING
DNS in Kubernetes (CoreDNS)
Kubernetes resources for coreDNS are:  
-a service account named coredns,
-cluster-roles named coredns and kube-dns
-clusterrolebindings named coredns and kube-dns, 
-a deployment named coredns,
-a configmap named coredns and a
-service named kube-dns.

This is the backend to k8s for cluster.local and reverse domains > proxy . /etc/resolv.conf

1. If you find CoreDNS pods in pending state first check network plugin is installed.

2. coredns pods have CrashLoopBackOff or Error state
If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the 
coredns pods are not starting. To solve that you can try one of the following options:
a)Upgrade to a newer version of Docker.
b)Disable SELinux.
c)Modify the coredns deployment to set allowPrivilegeEscalation to true:
kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.
There are many ways to work around this issue, some are listed here:
Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. 
For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the "real" resolv.conf, although this can be different depending on your distribution.
Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.
A quick fix is to edit your Corefile, replacing forward . /etc/resolv.conf with the IP address of your upstream DNS, 
for example forward . 8.8.8.8. But this only fixes the issue for CoreDNS, kubelet will continue to forward the invalid 
resolv.conf to all default dnsPolicy Pods, leaving them unable to resolve DNS.

3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.
        kubectl -n kube-system get ep kube-dns
If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.

Kube-Proxy
---------
kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. 
These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.
In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.
kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going 
to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods.
If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following 
command inside the kube-proxy container.
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)

  So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running.

 In the config file we define the clusterCIDR, kubeproxy mode, ipvs, iptables, bindaddress, kube-config etc.

Troubleshooting issues related to kube-proxy
1. Check kube-proxy pod in the kube-system namespace is running.
2. Check kube-proxy logs.
3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.
4. kube-config is defined in the config map.
5. check kube-proxy is running inside the container

~
#- What's the difference between ClusterIP, NodePort and LoadBalancer service types in Kubernetes?
A ClusterIP exposes the following:
spec.clusterIp:spec.ports[*].port
You can only access this service while inside the cluster. It is accessible from its spec.clusterIp port. 
If a spec.ports[*].targetPort is set it will route from the port to the targetPort. The CLUSTER-IP you get when 
calling kubectl get services is the IP assigned to this service within the cluster internally.

A NodePort exposes the following:
<NodeIP>:spec.ports[*].nodePort
spec.clusterIp:spec.ports[*].port
If you access this service on a nodePort from the nodes external IP, 
it will route the request to spec.clusterIp:spec.ports[*].port, which will in turn route it to your
spec.ports[*].targetPort, if set. This service can also be accessed in the same way as ClusterIP.
Your NodeIPs are the external IP addresses of the nodes. You cannot access your service from 
spec.clusterIp:spec.ports[*].nodePort.

A LoadBalancer exposes the following:
spec.loadBalancerIp:spec.ports[*].port
<NodeIP>:spec.ports[*].nodePort
spec.clusterIp:spec.ports[*].port
You can access this service from your load balancers IP address, which routes your request to a nodePort, 
which in turn routes the request to the clusterIP port. You can access this service as you would a NodePort 
or a ClusterIP service as well.

To clarify in a simple way:
ClusterIp exposure < NodePort exposure < LoadBalancer exposure

ClusterIp = Expose service through k8s cluster with ip/name:port
NodePort = Expose service through Internal network VMs also external to k8s ip/name:port
LoadBalancer = Expose service through External world or whatever you defined in your LB.
~
